{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwfusLBhhlla",
        "outputId": "96235b41-2ca4-490f-9818-b1b66e1da52b"
      },
      "outputs": [],
      "source": [
        "# ! python3 -m pip install natasha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8DlQzkuORrP"
      },
      "source": [
        "# **Named Entity Recognition (NER)**\n",
        "\n",
        "Воспользуемся готовыми инструментами для выделения именованных сущностей\n",
        "\n",
        "Создадим базовый пайплаин для NER\n",
        "- Воспользуемя библиотекой `natasha` и используем регулярные выражение `re`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bRwql78KSZl"
      },
      "source": [
        "## <div style=\"padding: 30px;color:white;margin:0px;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3B454F\"><b><span style='color:#FFFFFF'>1 |</span></b> <b>Библиотека natasha</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUdNJnSDaehB"
      },
      "source": [
        "### **1.1. Импортируем модуль**\n",
        "\n",
        "- Одна из библиотек которые можно использовать для выделения именованных сущностей это библиотека `natasha`\n",
        "- `natasha` это готовый инструмент для `NER`, работает только для русского языка\n",
        "- Посмотрим как этой библиотекой пользоваться для выделения именованных сущностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "59hnp6NPhllc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# загружаем все компоненты которые нам понадобятся\n",
        "from natasha import (Segmenter,MorphVocab,\n",
        "                     NewsEmbedding,NewsMorphTagger,NewsSyntaxParser,NewsNERTagger,\n",
        "                     PER,NamesExtractor,DatesExtractor,MoneyExtractor,AddrExtractor,\n",
        "                     Doc\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0YoGHx7iHfv"
      },
      "source": [
        "Инициализируем все наши компоненты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "x4VA-ajyhllf"
      },
      "outputs": [],
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "# dates_extractor = DatesExtractor(morph_vocab)\n",
        "# money_extractor = MoneyExtractor(morph_vocab)\n",
        "# addr_extractor = AddrExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFMfdBSIFdxx"
      },
      "source": [
        "### **1.2. Чтение данных**\n",
        "\n",
        "Читаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Pp4EY2Yxhllg"
      },
      "outputs": [],
      "source": [
        "with open('res.txt', 'r') as f:\n",
        "    text = f.read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "Ss_LbtoFhllh",
        "outputId": "414b5520-a59f-4678-94ed-bba2653e6b6c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Предположение о том, что мы произошли от обезьяны в 21 веке терпит крах. Ученые зашли в тупик. За годы исследований им так и не удалось доказать теорию эволюции и найти общих предков человека и приматов. Иначе современные шимпанзе или гориллы уже давно превратились бы в людей. Мы слишком не похожи друг на друга, чтобы быть родственниками. И даже генетики это подтверждают. Так что Дарвин был неправ. Никакого отношения человек к обезьянам не имеет. До сих пор ученые, биологи, палеонтологи, антропологи и прочие, и типа меня персонажи, вынуждены это опровергать. И поэтому нынче я, Станислав Дорбышевский, буду жарить этот дурацкий миф вместе с Рбака Трендами. На планете существует очень много млекопитающих. И эти млекопитающие по-разному родственны друг другу. Есть мерзуны, рукокрылые, хищные, копытные, китообразные, панголины и всякие прочие. А есть приматы. Приматов много разных. И удивительным образом, человек это просто один из великого множества этих самых приматов. И это родство, если еще в 18-19 веке было недалеко очевидно и не до конца очевидно, то за последние где-то лет сто уже более-менее разобрались. И главная группа самых живых организмов уже давно наведен в полный порядок. Отряд приматов характеризуется съедностью, древесностью, кастритейки размножения, когда мало детенышей долго выращиваются. Ну и на самом деле главное анатомия. То есть определенным набором зубов, строением зубов, той же самой генетикой, биохимией и так далее. И по всем этим параметрам человек это примат просто одна из обезьян. У нас тот же самый набор зубов и более того очень примитивный, очень такой простой незамысловатый. Ну базовое строение подобное было уже 40 миллионов лет назад на самом деле. Ну а порядка 26 миллионов лет назад сложился там этот самый узор дриапи-тега, так называемый на зубах. Ну такой гоминоидный, гоминидный даже. И в принципе с тех пор зубы принципиально не менялись. Немножко изменялись пропорции, размеры, там клыки стали чуть поменьше у нас. И наши зубы, по сути это чуть там как бы покрупнее зубы проконсула. Строение наших ручек, ножек тоже вообще совсем базовое. То есть весь набор костей у нас даже не срослась, малая берцовая, с большой, как это у долгопятов например произошло. У нас все то же самое, запястья, пястья, фаланги пальцев, ногти, все как у обезьян. И если мы посмотрим кисть того же самого проконсула или там еще кого-то более древнего даже, ну уж тем более более позднего, и человеческой разницы там будет немного. Да у нас на позднем этапе, два с половиной миллиона лет назад и даже позже там до полтора миллионов лет добавился трудовой комплекс кисть, но на фоне того, что существует других приматтов это ни о чем. Например если взять современных наших ближайших родственников гориллы шимпанзе, то мы увидим, что у них есть комплекс хождения на согнутых пальцах. Костяшкохождение этот комплекс гораздо более специфичен, чем наш трудовой на самом деле. Ну потому что он испытывает большие нагрузки до самой комплекса, самой эти кисти и соответственно гораздо более ответственный. А мы можем жить с трудовым комплексом, а можем в чем-то и без. То есть наша кисть очень проста, очень замасловата, а ежели брать какую-то отдельную кость из нее, ну допустим плечевую, то отличить человеческую от шимпанзиной крайне проблематично. И если это ископаемый кто-нибудь, какая-нибудь там австралопитек, то бывает очень трудно понять чья кость. Более того, в некоторых отношениях своей скелетной системы мы очень примитивны. Ну, например, у нас в копчиковом отделе четыре копчиковых позвонка, а у современного шимпанзе и даже у орнегутана в среднем три копчиковых позвонка. То есть мы хвостатее, чем шимпанзе, мы отстали от них, мы примитивные. И поэтому наша бесхвостость, это не то что человеческая черта, это человеческая черта, да, но она примитивная среди даже наших вот родственников антропоидов. Некоторые из нюансов нашей эволюции до сих пор еще находят какие-то дополнительные подтверждения, выслеживание, например, по генетике. Ну, например, вот совсем-таки недавно, в 2021, кажется, году, была чудесная статья про генетику исчезновения нашего хвоста. Довольно долго считалось, что хвост исчезал как бы постепенно по маленьку, потому что в начале был большой такой балансир, потом стал покороче-покороче-покороче и стал совсем маленький. Но было показано, что существует очень небольшое на самом деле количество мутаций и подробнейше описано, как все это происходило. Короче говоря, там буквально всего парочка мутаций, кроме того, что хвост бац и исчез. То есть вот они только же убегали с большими, а тут бац уже и с маленькими, вообще уже без хвоста. То есть на самом деле эволюционный процесс иногда может происходить реально очень быстро в течение буквально таких нескольких мутаций. Ну, если эти мутации дают какое-то преимущество или как минимум не оказываются вредными, а для первых человекообразных отсутствие хвоста не было вредным, потому что они уже не прыгали так далеко, как их предки по ветвям, и хвост в качестве балансира смысл потерял. И показано, что подобные же мутации бывают, например, у мышей. Вот, но мышам хвостик все-таки нужен, поэтому мыши обычно таки с хвостами. Некоторые маленькие мутации выражаются в больших внешних изменениях. Ну, например, редукция шерсти, как бы очевидно, что обезьяна волосатая, а мы типа лысые. Однако же, если посчитать количество волосяных луковиц на коже, то у человека шимпанзе оно примерно одинаковое. Потому что, например, на первый взгляд кажется, что у нас лоб совсем лысый, а если посчитать количество волосяных луковиц, там столько же, сколько на волосице части головы. И лоб-то у нас с мохнатой, на самом деле, это даже можно потрогать и приощутить, если аккуратненько-аккуратненько потрогать свой лобошник. Ну, а, например, волосатость груди у человека больше, чем у гориллы. У гориллы там вообще практически нет волос, а у человека очень даже есть. Поэтому мы тоже волосяты, на самом-то деле. Просто есть гены, генерирующие как бы сам белок, ну, программирующие сам белок, кератин, а есть гены, регулирующие, сколько этого кератина нарастет. И вот какой кератин – это одно и то же, что у нас шимпанзе, а сколько? У шимпанзе много получается, у нас этот ген поломанный, он нормально не регулирует и мало получается. Но и у нас, и у шимпанзе, например, нет подшорстка, и его нет вообще у всех человекообразных, очень-очень редко бывает у мартышкообразных, ну и там чуть более часто, но тоже редко, уши, раконосок каких-то обезьян. Генетика – это основа нашей наследственности, да, это вот АБВГД, в котором записано и биохимия, и строение, и анатомия, и даже поведение. И мы видим, что эта самая генетика нас и шимпанзе расходятся на какие-то жалоки, там 2,4%, потому что важно не сколько замен нуклеотидов, а что значат эти замены. У нас есть масса незначимых замен, а есть очень немного значимых, но которые всю разницу собственности составляют. И ориентируясь на генетику, мы опять же видим, что человека шимпанзе – это почти одно и то же, то есть между, скажем, индийским и африканским слоновами разница гораздо больше вообще-то, и вот поиск генетических разниц между человеком и обезьяной – это такая, скажем, непростая задача. Ну и обнаружившись, что там есть какие-то гены, ответственный зречь, развитие мозга. Буквально таки в 2022 году недавно была статья про то, как там некоторые очень маленькие мутации привели к тому, что наши мозги получили возможность бурно расти и распухать, и очень немногочисленные одноразовые, можно сказать, такие мутации могли иметь очень далекие последствия и привели нас на вершину эволюции, как нам кажется, по крайней мере, мы на вершине, и сделать нас умными, говорящими и все такое прочее. Поедение – тоже разница человека и нечеловека, ну казалось бы очевидно, мы такие шибко умные, говорящие, мегаобщительные, а на практике сводится к очень небольшому количеству свойств. Разницы, конечно, есть. Человек гораздо более спокойный, он контролирует свои какие-то вот эти позывы внутренние, то, что у него все-таки мышление развито чуть получше, он намного более занудный, он очень склонен учить и учиться, и склонен больше договариваться, нежели бить друг другу по мордасам, а шампанзе, ну примерно все то же самое, но в обратную сторону. То есть она агрессивнее, она менее спокойная, она быстро переключается, она учится просто наблюдением, но не целенаправленной, не склонна учить кого-то, ну зато на собственном опыте, а так по большому счету то же самое. И если, например, научить обезьян говорить, ну на языке жестов, например, то они говорят не то, что не хуже человека, но на уровне двухлетнего ребенка вполне себе, ну так у них и мозков три раза меньше, в общем-то. Так что различия получаются не качественные, а скорее такие количественные, и все главные свойства, наши поведения, которые мы склонны считать сугубо человеческими, доброта, юмор, способность к обману, ругательство даже, да, и прочее-прочее, у человека и у обезьян строго одинаковые, и уж особенно здорово, это видно по палеонтологическим данным, потому как уже 200 лет примерно палеонтологи выкапывают всяческие запчасти из земли, сравненных между собой, и нынче у нас есть полная последовательность от Пургаториуса, который жил 66 миллионов лет назад, который был размером с крысу, ну даже поменьше, там грамм 20 он там весил, ну там до 100 грамм, может с максимум, и с видом землеройки, и вот от этого самого Пургаториуса, и до нас есть все главные промежуточные формы. Где-то что-то у нас не до конца известное, в каких-то отрезках времени у нас, скажем, только зубы полно скелетов нет, да, потому что обезьяны прыгали по деревьям, и в палеонтологические келетки попадали, к сожалению, далеко не всегда, ну тем более, что большая часть истории их варилась где-то в тропиках, где просто палеонтологи не сильно копали, вот, но даже при всем этом у нас есть безумное количество палеонтологических данных, мы живем в чудесное время, когда мы не просто догадываемся, что были какие-то там промежуточные, там, недостающие звенья, когда мы их видим, просто в ручках можем поддержать и понять с какой стати они вылезали на деревья, как они слизались с деревьев, почему они меняли свою диету, брали в руки булыжники, начинали расселяться там из тропических зон куда-то, и такие вот замечательные находки показывают вот эти самые планненки переходы. Один из частейших таких аргументов тех, кто не верит в предосхождение человека от обезьяны, что, дескать, а почему же тогда сейчас современные обезьяны не превращаются в людей? Аргумент дебильный, потому что существует много разных экологических ниш и много разных эволюционных путей, и даже к одной экологической нише можно приспособиться по-разному, что зависит от каких-то сиюминутных нюансов, от наличия мутаций необходимых и от исторической истории, вот этой всей эволюционной истории, которая была до, и нету вообще никакой эволюционной силы, которая бы двигала кого-то в сторону человека, у каждого живого существования есть свой эволюционный путь, современные мартышки эволюци Statue эволюционировали в мартышек, потому что их предаки не совсем так, как мы, у них были другие предпосылки, у них были другие там нюансы, у них было другое поведение, у них были свои мутации, потому что у них были другие задачи, они по-другому питались, они другое運ались, они были другого размера, они сталкивались с другими конкурентами, некоторые из них эволюционировали вообще, на других каких-то землях, ну например,iskari, например, где условия все-таки совсем не такие, как у нас. Вот, а где-то там кто-то в юго-восточной азике, гибоны, где леса просто гущие, а кто-то в Африке, где сувенизация шла в полном рост и, соответственно, им там на землю надо было спускаться. Но те же гибоны от исходника отличаются сильно больше, чем мы. На самом деле, они гораздо более эволюционно продвинуты. Почему вы до сих пор, спрашивающие меня про обезьян, не вульсинировали в гибоны? Непорядок. Далеко еще вам до гибона, который является вершиной эволюции, у которого руки вот такой длины, мозги вот такие, они очень добрые, между прочим, и петь умеют очень хорошо. Сам по себе вот этот подход, что почему обезьяна не становится человеком, обезьян много разных. Есть руконожки, есть кольцехвостные лемуры, есть какие-нибудь толстые лори, а есть тонкие лори, а есть много пятые, а есть макайки такие, есть сики, есть павианы, есть гибоны, орангутаны, горилла, шимпанзе. И каждая из этих тварей, она прошла свой отдельный путь. Я уж помолчу еще про сон ископаемых приматов, которых вообще немеренное количество, которые шли еще своими какими-то путями. Да даже люди эволюционировали сильно и не одинаково на самом деле, потому что окроме нас были еще няндертальцы параллельно, диннисовцы, хоббиты флорецкие, хоббиты лусонские, какие-то там западноафриканские, эти непонятные там, ивэлэруэйдинсис, и там, бог знает, кто еще, ну даже банально, я не знаю, даже среди современного это человечество и то, разнообразие велико. Вот почему все не эволюционируют в австралийских аборигенов? Ну потому что не все живут в Австралии и жили в прошлом, да? Поэтому если мы будем подходить чисто практически, да, как вот в современности, например, юриспунденции устанавливается родство людей, отцовство, например, да, кто папа, кто мама, кто дедушка, бабушка, то окажется, что да, это такие человек-обезьяны. Можно, конечно, игнорировать, это легко, вот, но можно не знать, ну если не интересоваться вообще, но если иметь хоть минимальное желание, это такая прорва информации, что в нем можно булькнуться, погрузиться с ногами, с головой, с ушами и всю жизнь там кувыркаться в этой информации. Чем я, например, занимаюсь и спорить о том, что человек не произошел от обезьяны, ну как-то даже наивно. Если уж мы есть обезьяны, то куда деваться? Вот мы особенные обезьяны, мы можем думать о других обезьянах, изучать других обезьян и планировать свое обезьянье будущее, чем мы замечательны. Так что занимайтесь лучше этим, а не бредом, а не всякими мифами.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf_KzOjNhlli",
        "outputId": "438eaeb0-470d-474b-c775-c6d6d1aa5296"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13899"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9osxSbFBZLS"
      },
      "source": [
        "### **1.3. `natasha` document**\n",
        "\n",
        "Иницилизируем класс которые будет хранить все данные о тексте, в отличие от `spacy`, doc это не готовая pipeline\n",
        "\n",
        "Нам надо будет вызывать отделные методы для того чтобы заполнить содержание которое нас интересует:\n",
        "- `.segmenter` - токенизация, разбиваем на предложение\n",
        "- `.tag_morph` - морфологический разбор токенов (e.g. NOUN)\n",
        "- `.tag_ner` - выделение именованных сущностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViaEcXvAhllk",
        "outputId": "cd9deaa2-08de-4cb4-b8af-247041610e44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Doc(text='Предположение о том, что мы произошли от обезьяны...)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc = Doc(text)\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guvx4i62cyVF",
        "outputId": "f3d4aa9b-f9f6-4ad6-b64a-a6e22d375ebf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['as_json',\n",
              " 'clear_envelopes',\n",
              " 'envelop_sent_spans',\n",
              " 'envelop_sent_tokens',\n",
              " 'envelop_span_tokens',\n",
              " 'from_json',\n",
              " 'morph',\n",
              " 'ner',\n",
              " 'parse_syntax',\n",
              " 'segment',\n",
              " 'sents',\n",
              " 'spans',\n",
              " 'syntax',\n",
              " 'tag_morph',\n",
              " 'tag_ner',\n",
              " 'text',\n",
              " 'tokens']"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[a for a in dir(doc) if not a.startswith('_')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-mykzD9fhlll"
      },
      "outputs": [],
      "source": [
        "# vars(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyi1JC-8mRp1"
      },
      "source": [
        "### **1.4. Разбиваем документ на предложения и токены**\n",
        "\n",
        "Разбиваем текста на токены (`tokens`) и предложения (`sents`), данные сохраняются в `doc`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "RLTfh3IPhllm",
        "outputId": "277b3324-e7c5-453e-e522-a5a5fd45721c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Doc(text='Предположение о том, что мы произошли от обезьяны..., tokens=[...], sents=[...])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sentences'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[DocSent(stop=72, text='Предположение о том, что мы произошли от обезьяны..., tokens=[...]),\n",
              " DocSent(start=73, stop=94, text='Ученые зашли в тупик.', tokens=[...]),\n",
              " DocSent(start=95, stop=203, text='За годы исследований им так и не удалось доказать..., tokens=[...])]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tokens'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[DocToken(stop=13, text='Предположение'),\n",
              " DocToken(start=14, stop=15, text='о'),\n",
              " DocToken(start=16, stop=19, text='том'),\n",
              " DocToken(start=19, stop=20, text=','),\n",
              " DocToken(start=21, stop=24, text='что')]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Добавляем в doc tokens, sents\n",
        "doc.segment(segmenter)\n",
        "\n",
        "display(doc) # документ\n",
        "display('sentences',doc.sents[:3]) # предложения\n",
        "display('tokens',doc.tokens[:5])  # токены"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc5r9tKbndMV"
      },
      "source": [
        "### **1.5. Выделяем часть речи**\n",
        "\n",
        "Вызовим метод `tag_morph` и передадим объект `morph_tagger`\n",
        "- Сделаем морфологический разбор документа\n",
        "- \"POS\" - часть речи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "WHTiNryRhllo",
        "outputId": "5811bb26-db66-4c50-9bb5-37e1c53cbeb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DocToken(stop=13, text='Предположение', id='1_1', head_id='1_13', rel='nsubj', pos='NOUN', feats=<Inan,Acc,Neut,Sing>),\n",
              " DocToken(start=14, stop=15, text='о', id='1_2', head_id='1_3', rel='case', pos='ADP'),\n",
              " DocToken(start=16, stop=19, text='том', id='1_3', head_id='1_1', rel='nmod', pos='PRON', feats=<Inan,Loc,Neut,Sing>),\n",
              " DocToken(start=19, stop=20, text=',', id='1_4', head_id='1_7', rel='punct', pos='PUNCT'),\n",
              " DocToken(start=21, stop=24, text='что', id='1_5', head_id='1_7', rel='mark', pos='SCONJ'),\n",
              " DocToken(start=25, stop=27, text='мы', id='1_6', head_id='1_7', rel='nsubj', pos='PRON', feats=<Nom,Plur,1>),\n",
              " DocToken(start=28, stop=37, text='произошли', id='1_7', head_id='1_3', rel='acl', pos='VERB', feats=<Perf,Ind,Plur,Past,Fin,Act>),\n",
              " DocToken(start=38, stop=40, text='от', id='1_8', head_id='1_9', rel='case', pos='ADP'),\n",
              " DocToken(start=41, stop=49, text='обезьяны', id='1_9', head_id='1_7', rel='obl', pos='NOUN', feats=<Anim,Gen,Fem,Sing>),\n",
              " DocToken(start=50, stop=51, text='в', id='1_10', head_id='1_12', rel='case', pos='ADP'),\n",
              " DocToken(start=52, stop=54, text='21', id='1_11', head_id='1_12', rel='amod', pos='ADJ'),\n",
              " DocToken(start=55, stop=59, text='веке', id='1_12', head_id='1_13', rel='obl', pos='NOUN', feats=<Inan,Loc,Masc,Sing>),\n",
              " DocToken(start=60, stop=66, text='терпит', id='1_13', head_id='1_3', rel='acl', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>),\n",
              " DocToken(start=67, stop=71, text='крах', id='1_14', head_id='1_13', rel='obj', pos='NOUN', feats=<Inan,Acc,Masc,Sing>),\n",
              " DocToken(start=71, stop=72, text='.', id='1_15', head_id='1_13', rel='punct', pos='PUNCT')]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# выделяем части речи\n",
        "doc.tag_morph(morph_tagger)\n",
        "doc.parse_syntax(syntax_parser)\n",
        "display(doc.tokens[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ttG8cvvophb"
      },
      "source": [
        "### **1.6. Выделяем именнованые сущности**\n",
        "- Для выделения именованных сущностей в документе вызываем `tag_ner`\n",
        "- Именованные сущности сохраняются в `.spans`\n",
        "    - В `spans.type` сохраняется класс именованной сущности\n",
        "    - В `spans.text` сохраняется выделенная именнованая сущность"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExXki_pJhllq",
        "outputId": "4c326732-6d58-4608-de43-2e9a486b1422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "len(doc.spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgJGDt9Thlls",
        "outputId": "5945500b-a0f9-483f-9fd6-e6319b9df3a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DocSpan(start=383, stop=389, type='PER', text='Дарвин', tokens=[...]),\n",
              " DocSpan(start=584, stop=606, type='PER', text='Станислав Дорбышевский', tokens=[...]),\n",
              " DocSpan(start=647, stop=661, type='PER', text='Рбака Трендами', tokens=[...]),\n",
              " DocSpan(start=7655, stop=7663, type='PER', text='Поедение', tokens=[...]),\n",
              " DocSpan(start=9141, stop=9153, type='PER', text='Пургаториуса', tokens=[...]),\n",
              " DocSpan(start=9349, stop=9361, type='PER', text='Пургаториуса', tokens=[...]),\n",
              " DocSpan(start=11547, stop=11553, type='LOC', text='Африке', tokens=[...]),\n",
              " DocSpan(start=12972, stop=12981, type='LOC', text='Австралии', tokens=[...])]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGgs9bKDhllv",
        "outputId": "ce7655b7-d416-494d-ba6e-4ffa1709d0bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Австралии',\n",
              " 'Африке',\n",
              " 'Дарвин',\n",
              " 'Поедение',\n",
              " 'Пургаториуса',\n",
              " 'Рбака Трендами',\n",
              " 'Станислав Дорбышевский'}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set([s.text for s in doc.spans])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxI669SvMGzF",
        "outputId": "484f8052-e1ad-4144-867f-35629a7e3344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Дарвин PER\n",
            "Станислав Дорбышевский PER\n",
            "Рбака Трендами PER\n",
            "Поедение PER\n",
            "Пургаториуса PER\n",
            "Пургаториуса PER\n",
            "Африке LOC\n",
            "Австралии LOC\n"
          ]
        }
      ],
      "source": [
        "for span in doc.spans:\n",
        "    print(span.text,span.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI4AG0BTLLSW"
      },
      "source": [
        "Можно лемматизировать содержание используя `.normalize`, данные сохраняются в `span.normal`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6RjwQNChllw",
        "outputId": "e05f7d92-ba10-4414-d4c6-4ab3933cbb0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Дарвин': 'Дарвин',\n",
              " 'Станислав Дорбышевский': 'Станислав Дорбышевский',\n",
              " 'Рбака Трендами': 'Рбака Тренды',\n",
              " 'Поедение': 'Поедение',\n",
              " 'Пургаториуса': 'Пургаториус',\n",
              " 'Африке': 'Африка',\n",
              " 'Австралии': 'Австралия'}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# лемматизируем извлеченные сущности\n",
        "for span in doc.spans:\n",
        "    span.normalize(morph_vocab)\n",
        "\n",
        "{s.text: s.normal for s in doc.spans}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKM-jvKyhlly",
        "outputId": "5a52f7b7-448c-4859-b534-d8ead005f5f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Дарвин': {'last': 'Дарвин'},\n",
              " 'Станислав Дорбышевский': {'first': 'Станислав', 'last': 'Дорбышевский'},\n",
              " 'Рбака Тренды': {'last': 'Рбака'}}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# извлечем персон\n",
        "for span in doc.spans:\n",
        "    if(span.type == PER):\n",
        "        span.extract_fact(names_extractor)\n",
        "\n",
        "{s.normal: s.fact.as_dict for s in doc.spans if s.fact}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPMosdkHhll0"
      },
      "source": [
        "## <div style=\"padding: 30px;color:white;margin:0px;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#3B454F\"><b><span style='color:#FFFFFF'>2 |</span></b> <b>Объединенный метод NER</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skjms-EgaXRP"
      },
      "source": [
        "\n",
        "### **2.1. Подготовка данных**\n",
        "\n",
        "`natasha` может упускать именованные сущности, мы можем использовать результаты из `re`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pwzTuwnUhll3"
      },
      "outputs": [],
      "source": [
        "# лемматизация\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "from functools import lru_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG57O1Flhll6",
        "outputId": "1be8cc4d-0c59-40fd-aacf-abe412388722"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['res.txt', 'res2.txt']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_folder = '.'\n",
        "files = os.listdir(dataset_folder)\n",
        "files = [i for i in files if '.txt' in i]\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VJxnKSVbhll7"
      },
      "outputs": [],
      "source": [
        "texts = []\n",
        "for f in files:\n",
        "    with open(os.path.join(dataset_folder, f), 'r') as fo:\n",
        "        texts.append(fo.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IutwcIWg68c"
      },
      "source": [
        "### **2.2. Вспомогательные функции**\n",
        "\n",
        "(1) **NER** с `natasha`\n",
        "\n",
        "Вспомогательная функция для NER с `natasha`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "R9bE1UOUhll9"
      },
      "outputs": [],
      "source": [
        "def get_ner_natasha(text):\n",
        "    text = re.sub(r'[^s\\d\\w\\-:,\\.\\?\\!]', ' ', text) # убираем пунктуация\n",
        "    doc = Doc(text)                                 # создаем natasha документ\n",
        "    doc.segment(segmenter)                          # tokenise, sentences\n",
        "    doc.tag_ner(ner_tagger)                         # tag NER\n",
        "\n",
        "    # normalise using morpher\n",
        "    for span in doc.spans:\n",
        "        span.normalize(morph_vocab)\n",
        "\n",
        "    res = set((s.normal for s in doc.spans))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLiqJvJb0YYY"
      },
      "source": [
        "(2) **NER** с `re`\n",
        "\n",
        "Воспомогательная функция для NER с `re`, воспользуемся регулярками и лемматизатора из `pymorphy2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "gQobAeo-hll-"
      },
      "outputs": [],
      "source": [
        "@lru_cache(10000)\n",
        "def lemmatize(s):\n",
        "    s = str(s).lower()\n",
        "    return morph.parse(s)[0].normal_form.capitalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HGmPTm_Hhll_"
      },
      "outputs": [],
      "source": [
        "reg1 = re.compile(r'[^s\\d\\w\\-:,\\.\\?\\!]')\n",
        "reg2 = re.compile(r'([\\.\\?!])')\n",
        "\n",
        "def get_ner_regex(s):\n",
        "    s = reg1.sub(' ', s)\n",
        "    s = reg2.sub(r'\\g<1><sep>', s)\n",
        "\n",
        "    # разбиваем на предложения\n",
        "    sent1 = [sent.strip() for sent in s.split('<sep>')]\n",
        "    sent2 = [' '.join(ss.split()[1:]) for ss in sent1]\n",
        "\n",
        "    res = []\n",
        "    for ss in sent2:\n",
        "        res.extend([e.strip() for e in re.findall(r'(?:[A-ZА-ЯЁ][A-ZА-ЯЁа-яёa-z\\d-]+\\s*)+', ss)])\n",
        "\n",
        "    return set((lemmatize(s) for s in res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_MMq8Np0JVx"
      },
      "source": [
        "(3) Объединим результаты из `natasha` с `re`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bcDGaoONhlmA"
      },
      "outputs": [],
      "source": [
        "def get_ner(text):\n",
        "    return get_ner_regex(text).union(get_ner_natasha(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5fBrXYGtQav"
      },
      "source": [
        "### **2.3. Проверка подхода**\n",
        "\n",
        "Подтвердим что все работает по отдельности\n",
        "- `get_ner_natasha` нам возвращает сущности которые выделила `natasha`\n",
        "- `get_ner_regex` нам возвращает сущности которые выделил `re`\n",
        "- `get_ner` нам возващает объединение двух подходов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz6HAnjuhlmB",
        "outputId": "dbb979d0-be07-45e7-994f-ce8278c2e8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ракета-носитель «Союз-СТ-А» успешно запущена из Гвианского космического центра 26 апреля в 0:02 по московскому времени. На борту находились спутники Sentinel-1B, Microscope и FYS, сообщает Роскосмос.\n",
            "Sentnel-1B предназначен для наблюдения за сушей и океанами в радиодиапазоне. Спутник идентичен запущенному двумя годами ранее Sentinel-1A, оба аппарата Sentinel-1 будут работать в паре, собирая данные с противоположных точек орбиты. Sentinel-1 помогают следить за состоянием океанов, ледников и лесов. Получаемые с орбиты данные используются для обнаружения айсбергов и нефтяных разливов, а также для предоставления актуальных картографических данных при чрезвычайных ситуациях. Кроме того, данные со спутников используются в различных «непрофильных» исследованиях. Например, группа немецких ученых опубликовала исследование о предполагаемом месте испытаний северокорейского ядерного и термоядерного оружия, в работе использовался и интерферометрический снимок предполагаемого места, сделанный Sentinel-1A.\n",
            "Запуск спутников группировки Sentinel производится в рамках проекта мониторинга окружающей среды «Коперник», созданного Европейской комиссией совместно с Европейским космическим агентством. На протяжении ближайших дней инструменты спутника будут откалиброваны и зонд приступит к работе.\n",
            "Первый спутник группировки Sentinel был выведен на орбиту 3 апреля 2014, всего к запуску планируется шесть разных модификаций Sentinel с различной специализацией. Предыдущий спутник Sentinel-3A был выведен на орбиту в феврале 2016 года.\n",
            "Николай Воронцов\n"
          ]
        }
      ],
      "source": [
        "text = texts[1]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIB6DhHWhlmC",
        "outputId": "55792637-b600-42ff-9cbd-8b4f301bbefc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'FYS',\n",
              " 'Гвианского космического центра',\n",
              " 'Европейским космическим агентством',\n",
              " 'Европейской комиссией',\n",
              " 'Коперник',\n",
              " 'Николай Воронцов',\n",
              " 'Роскосмос'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NER который нашел natasha\n",
        "get_ner_natasha(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRkURJMZhlmD",
        "outputId": "a0655f1b-5021-49a9-d131-7f655cf899fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fys',\n",
              " 'Microscope',\n",
              " 'Sentinel',\n",
              " 'Sentinel-1',\n",
              " 'Sentinel-1a',\n",
              " 'Sentinel-1b',\n",
              " 'Sentinel-3a',\n",
              " 'Воронцов',\n",
              " 'Гвианский',\n",
              " 'Европейский',\n",
              " 'Коперник',\n",
              " 'Роскосмос',\n",
              " 'Союз-ст-'}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NER который нашел re\n",
        "get_ner_regex(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr4ZgpXyhlmE",
        "outputId": "61d8ca2a-1c4d-450c-e0e3-5f03d72f7339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'FYS',\n",
              " 'Fys',\n",
              " 'Microscope',\n",
              " 'Sentinel',\n",
              " 'Sentinel-1',\n",
              " 'Sentinel-1a',\n",
              " 'Sentinel-1b',\n",
              " 'Sentinel-3a',\n",
              " 'Воронцов',\n",
              " 'Гвианский',\n",
              " 'Гвианского космического центра',\n",
              " 'Европейский',\n",
              " 'Европейским космическим агентством',\n",
              " 'Европейской комиссией',\n",
              " 'Коперник',\n",
              " 'Николай Воронцов',\n",
              " 'Роскосмос',\n",
              " 'Союз-ст-'}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# объединим подходы\n",
        "get_ner(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z-Fxl2JF_mo"
      },
      "source": [
        "### **2.4. Работа на реальном примере**\n",
        "\n",
        "Все нормально работает, теперь возмем наш основной корпус документов (отзывы о банке)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MziSgVCLYufR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_review = pd.read_csv('review_cleaned.csv',usecols=['user','review_cleaned'])\n",
        "user_review = list(df_review['user'].values)\n",
        "actual_review = list(df_review['review_cleaned'].values)\n",
        "\n",
        "user_review = user_review[:50]\n",
        "actual_review = actual_review[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LaKXuqmhlmG",
        "outputId": "d3a1f06e-2a43-463d-dcd1-05ec0a6ae00d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:02, 24.78it/s]\n"
          ]
        }
      ],
      "source": [
        "res_ners = []\n",
        "for user, text in tqdm(zip(user_review,actual_review)):\n",
        "    res_ners.append((user, sorted(get_ner(text))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjY6rKilAdN"
      },
      "source": [
        "Пример NER для одного отзыва"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAza1xkchlmG",
        "outputId": "e8869238-362f-452d-f51b-aeebd222018f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['MasterCard',\n",
              " 'MasterCard Standard',\n",
              " 'Mastercard',\n",
              " 'Mastercard mass',\n",
              " 'Mastercard standard',\n",
              " 'Вы',\n",
              " 'Евгении',\n",
              " 'Евгений',\n",
              " 'Евгения',\n",
              " 'МИР']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_ners[0][1][:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
