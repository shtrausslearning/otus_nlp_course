# Векторные представления слов и работа с предобученными эмбедингами

### **Краткое содержание**
`word2vec`, `fasttex`, предобученные `эмбеддинги` 

### **Цели занятия**

- рассмотреть алгоритмы `word2vec`, `fasttext` 
- научиться работать с векторными представлениями слов
- использовать предобученные векторные представления для решения NLP-задач

### **Компетенции**

Анализ текстовых данных NLP
- решать задачи по автоматической обработке текстов (NLP)
- работать с векторными представлениями слов
- знать алгоритмы `word2vec` и `fasttext`

### **Практика**

В таблице указано содержание различных проэктов по этой теме 
|<code>name</code>|<code>content</code>|<code>conclusion</code>|
|-----------------|:-------------------|:----------------------| 
| **[embeddings_visualisation](https://github.com/shtrausslearning/otus_nlp_course/blob/main/3_Классические%20методы%20NLP/10_Векторные%20представления%20слов%20и%20работа%20с%20предобученными%20эмбедингами/text_embeddings.ipynb)** | <br> <sub>![](https://img.shields.io/badge/Project-Information-4169E1)</sub> В этом проекте мы сделаем предобработку и создадим векторное представление слов для текстовых данных из соцсетей. Обучим модели из библиотеки `gensim` и визуализируем этих вектора в двухмерном пространстве. <br><br> <sub>![](https://img.shields.io/badge/Input-Data-DE3163)</sub> Данные предоставлены в формате <code>csv</code> в отдельных файлах, конкатерируем их в помощью <code>pandas</code> <br><br> <sub>![](https://img.shields.io/badge/Text-Preprocessing-9ACD32)</sub> Для предобработки текстовых данных воспользуемся регулярками `re` либо используя лемматизацию (в этот раз используя библиотеку `pymystem3`) для чистки текста и прелбразование в числовое представление обучаем `word2vec` на нашем корпусе <br><br> <sub>![](https://img.shields.io/badge/Modeling-Process-3CB371)</sub> Для визуализации эмбедингов, сначала воспользуемся методом понижения размерности `tnse`, получим двухмерное представление 100 мерного представления слов, после чего используем кластеризацию для групирования похожих слов используя метод <code>AgglomerativeClustering</code> из библиотеки <code>sklearn</code>. Для визуализации воспользуемся интерактивной библиотекой `bokeh`. Далее воспользуемся эмбедингами ранее предобученными векторами, загужаем их в модель `fasttext` и визуализируем из двухмерное представление <br><br> <sub>![](https://img.shields.io/badge/Library-Stack-FFBF00)</sub> <code>re</code>,<code>pymystem3</code>,<code>sklearn</code>,<code>collections</code>,<code>pandas</code>, <code>gensim</code>, <code>bokeh</code> <br><br> <sub>![](https://img.shields.io/badge/GitHub-Gist-DA70D6)</sub>       | Цель этого ноутбука в том чтобы показать как можно создавать свои эмбединги слов, обучив модель `word2vec` на предоставленной ей корпусе эмбедини будут нести какой то семантический смысл, кодируя эту информацию в веторном виде. Узнали что мы можем визуализировать их используя методы неконтролируемого обучения и даже можем загружать ранее предобученные векторные преставление. После обработки текста мы конечно можем использовать эти эмбединги в каких то downstream task, например для классификации тональности документов |
