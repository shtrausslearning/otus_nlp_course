# Предобработка данных и понятие векторных представлений слов

### **Краткое содержание**
- токенизация; стоп-слова; нормализация, векторные представления, <code>BOW</code>, <code>TF-IDF</code>

### **Цели занятия**

- обсудить <code>pipeline</code> предобработки текстовых данных (удаление стоп-слов, токенизация, лемматизация)
- рассмотреть библиотеки для работы работы с текстом такие как <code>nltk</code> и `pymophy2`

### **Компетенции**

Анализ текстовых данных NLP
- использовать NLP библиотеки для работы с русским и английским языками 
- работать с векторными представлениями слов <code>tf-idf</code>

### **Практика**

В таблице указано содержание различных проэктов по этой теме 

| <code>name</code> | <code>content</code>                                                                                                        | <code>conclusion</code> |
|-----------------------|:-----------------------------------------------------------------------------------------------------------------------------|:-------------|
| **[Анализ Тональности Казахских Новостей](https://github.com/shtrausslearning/otus_nlp_course/blob/main/3_Классические%20методы%20NLP/9_Предобработка%20данных%20и%20понятие%20векторных%20представлений%20слов/khazah-news-sentiment.ipynb)** | <br> ![](https://img.shields.io/badge/Project-Information-4169E1) <br><br> В этом проэкте мы строим модели классического машинного обучения для предсказывания анализа тональности Казахских новостей, посмотрим какой подход векторизации текста покажет лучше результат на тестовой выборке <br><br> ![](https://img.shields.io/badge/Input-Data-DE3163) <br><br> Данные предоставлены в формате `json`, пример работы с этим форматом можно найти в **[text_normalisation](https://github.com/shtrausslearning/otus_nlp_course/blob/main/3_Классические%20методы%20NLP/9_Предобработка%20данных%20и%20понятие%20векторных%20представлений%20слов/text_normalisation.md)** <br><br> ![](https://img.shields.io/badge/Text-Preprocessing-9ACD32) <br><br> Для предобработки текстовых данных воспользуемся <code>Re</code>, токенизируем с помощью `WordPunctTokenizer`, удаляем стоп слов из `nltk` (и добавляем дополнительные), приводим слова в базовую форму используя `pymorphy2`. Для энкодинг текста воспользуемся методами <code>Bow</code>  и <code>TF-IDF</code> из <code>sklearn</code> (сравниваем оба подхода). Для ограничения размерности матрицы векторного представления используем max_features = 1000 <br><br> ![](https://img.shields.io/badge/Modeling-Process-3CB371) <br><br> Для классификатора воспользуемся случайным лесом (<code>RandomForestClassifier</code>), для гиперпараметров построим 500 решающих деревьев, другие параметры по умолчанию. Для проверки обобщаюшию способность модели воспользуемся методом <code>train_test_split</code>, тренируем модель на 80% данных, на остальных валидируем, для оценки модели используем <code>f1_score</code> с опции macro, для понимания как влияем дисбаланс классов <br><br> ![](https://img.shields.io/badge/Library-Stack-FFBF00) <br><br> <code>nltk</code>,<code>pymorphy</code>,<code>sklearn</code>,<code>json</code>,<code>pandas</code> <br><br> ![](https://img.shields.io/badge/GitHub-Gist-DA70D6)  <br><br>   |  ![](https://img.shields.io/badge/Model-Generalisation-8A2BE2)  <br><br> Результаты оценки моделей на тестовой выборке: <br> <li>BoW `F1` 0.610 </li><li>TF-IDF `F1` 0.642</li> <br>По сравнению с `BoW`, `TF-IDF` существенно улучшил качество модели, на текущем этапе модели не готовы к использованию на практике но разница между двумя подходами показательна |

