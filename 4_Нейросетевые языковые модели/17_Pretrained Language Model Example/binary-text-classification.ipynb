{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wget","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:20.900469Z","iopub.execute_input":"2023-07-12T14:21:20.900920Z","iopub.status.idle":"2023-07-12T14:21:32.200527Z","shell.execute_reply.started":"2023-07-12T14:21:20.900884Z","shell.execute_reply":"2023-07-12T14:21:32.199147Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wget in /opt/conda/lib/python3.10/site-packages (3.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:32.203895Z","iopub.execute_input":"2023-07-12T14:21:32.204267Z","iopub.status.idle":"2023-07-12T14:21:33.839789Z","shell.execute_reply.started":"2023-07-12T14:21:32.204228Z","shell.execute_reply":"2023-07-12T14:21:33.838797Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import wget\nimport os\nimport pandas as pd\n\nprint('Downloading dataset...')\n\n# The URL for the dataset zip file.\nurl = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n\n# Download the file (if we haven't already)\nif not os.path.exists('./cola_public_1.1.zip'):\n    wget.download(url, './cola_public_1.1.zip')\n    \n# Unzip the dataset (if we haven't already)\nif not os.path.exists('./cola_public/'):\n    !unzip cola_public_1.1.zip\n    \n\n# Load the dataset into a pandas dataframe.\ndf = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", \n                delimiter='\\t', \n                header=None, \n                names=['sentence_source', 'label', 'label_notes', 'sentence'])\n\n# Report the number of sentences.\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n\n# Display 10 random rows from the data.\ndf.sample(10)\n\n# В HuggingFace работаем с списками\nsentences = df.sentence.values\nlabels = df.label.values","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-12T14:21:33.842200Z","iopub.execute_input":"2023-07-12T14:21:33.843464Z","iopub.status.idle":"2023-07-12T14:21:33.874673Z","shell.execute_reply.started":"2023-07-12T14:21:33.843424Z","shell.execute_reply":"2023-07-12T14:21:33.873698Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading dataset...\nNumber of training sentences: 8,551\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokeniser = AutoTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:33.877410Z","iopub.execute_input":"2023-07-12T14:21:33.878010Z","iopub.status.idle":"2023-07-12T14:21:34.831941Z","shell.execute_reply.started":"2023-07-12T14:21:33.877976Z","shell.execute_reply":"2023-07-12T14:21:34.830888Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Найдем максимальную длину в документах (включая спец токены)\ndef find_max_sent_len(tokenizer):\n    max_len = 0\n    for sent in sentences:\n        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n        max_len = max(max_len, len(input_ids))\n\n    print('Max sentence length: ', max_len)\n\n# Токенизируем документы\ndef tokenise_sentences(tokenizer,sentences,labels):\n\n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in sentences:\n\n        encoded_dict = tokenizer.encode_plus(\n                            sent,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = 64,           # Pad & truncate all sentences.\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the list.\n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    # Print sentence 0, now as a list of IDs.\n    print('Original: ', sentences[0])\n    print('Token IDs:', input_ids[0])\n    print('label:',labels[0])\n    \n    return input_ids,attention_masks,labels\n    \nfrom transformers import AutoTokenizer\n\ntokeniser = AutoTokenizer.from_pretrained('bert-base-uncased')\ninput_ids,attention_masks,labels = tokenise_sentences(tokeniser,sentences,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:34.833440Z","iopub.execute_input":"2023-07-12T14:21:34.834059Z","iopub.status.idle":"2023-07-12T14:21:37.036278Z","shell.execute_reply.started":"2023-07-12T14:21:34.834022Z","shell.execute_reply":"2023-07-12T14:21:37.035217Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Original:  Our friends won't buy this analysis, let alone the next one we propose.\nToken IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\nlabel: tensor(1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\ndef make_tts(input_ids,attention_masks,labels):\n\n    # Имея input_ids, attention_masks, labels\n\n    # Combine the training inputs into a TensorDataset.\n    dataset = TensorDataset(input_ids, attention_masks, labels)\n\n    # Calculate the number of samples to include in each set.\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n\n    # Divide the dataset by randomly selecting samples.\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    print('{:>5,} training samples'.format(train_size))\n    print('{:>5,} validation samples'.format(val_size))\n    \n    return train_dataset,val_dataset\n    \ntrain_dataset,val_dataset = make_tts(input_ids,attention_masks,labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:37.037942Z","iopub.execute_input":"2023-07-12T14:21:37.038589Z","iopub.status.idle":"2023-07-12T14:21:37.047489Z","shell.execute_reply.started":"2023-07-12T14:21:37.038552Z","shell.execute_reply":"2023-07-12T14:21:37.046512Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"7,695 training samples\n  856 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\ndef make_dl(train_dataset,val_dataset,batch_size):\n\n    tdl = DataLoader(train_dataset,\n                    sampler = RandomSampler(train_dataset),\n                    batch_size = batch_size)\n\n    vdl = DataLoader(val_dataset,\n                     sampler = SequentialSampler(val_dataset),\n                     batch_size = batch_size)\n    \n    return tdl,vdl\n\ntrain_dataloader,validation_dataloader = make_dl(train_dataset,val_dataset,batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:37.048874Z","iopub.execute_input":"2023-07-12T14:21:37.049652Z","iopub.status.idle":"2023-07-12T14:21:37.062410Z","shell.execute_reply.started":"2023-07-12T14:21:37.049619Z","shell.execute_reply":"2023-07-12T14:21:37.061481Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n                                                           num_labels=2)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:37.063766Z","iopub.execute_input":"2023-07-12T14:21:37.064234Z","iopub.status.idle":"2023-07-12T14:21:45.748596Z","shell.execute_reply.started":"2023-07-12T14:21:37.064196Z","shell.execute_reply":"2023-07-12T14:21:45.747429Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8 )\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:45.750061Z","iopub.execute_input":"2023-07-12T14:21:45.750677Z","iopub.status.idle":"2023-07-12T14:21:45.763584Z","shell.execute_reply.started":"2023-07-12T14:21:45.750642Z","shell.execute_reply":"2023-07-12T14:21:45.762663Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\nimport numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:21:45.774164Z","iopub.execute_input":"2023-07-12T14:21:45.774538Z","iopub.status.idle":"2023-07-12T14:21:45.785442Z","shell.execute_reply.started":"2023-07-12T14:21:45.774507Z","shell.execute_reply":"2023-07-12T14:21:45.784432Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\n\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndef evaluate_text_classifier(train_dl,val_dl,epochs):\n\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n\n        t0 = time.time()\n        total_train_loss = 0\n\n        model.train()\n        for step, batch in enumerate(train_dl):\n\n            if step % 40 == 0 and not step == 0:\n                elapsed = format_time(time.time() - t0)\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dl), elapsed))\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()    \n            res = model(b_input_ids,\n                        token_type_ids=None,\n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n\n            loss = res['loss']\n            logits = res['logits']\n\n            total_train_loss += loss.item()\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n\n        # усредненный loss по батчам\n        avg_train_loss = total_train_loss / len(train_dl)\n        training_time = format_time(time.time() - t0)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epcoh took: {:}\".format(training_time))\n\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n\n\n        model.eval()\n        total_eval_accuracy = 0\n        total_eval_loss = 0\n        nb_eval_steps = 0\n\n        for batch in val_dl:\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            with torch.no_grad():\n\n                res = model(b_input_ids,\n                            token_type_ids=None,\n                            attention_mask=b_input_mask,\n                            labels=b_labels)\n\n            loss = res['loss']\n            logits = res['logits']\n\n\n            total_eval_loss += loss.item()\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n        avg_val_accuracy = total_eval_accuracy / len(val_dl)\n        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n        avg_val_loss = total_eval_loss / len(val_dl)\n        validation_time = format_time(time.time() - t0)\n\n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n\n        training_stats.append(\n            {\n                'epoch': epoch_i + 1,\n                'Training Loss': avg_train_loss,\n                'Valid. Loss': avg_val_loss,\n                'Valid. Accur.': avg_val_accuracy,\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n\n    print(\"\")\n    print(\"Training complete!\")\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n    df_stats = pd.DataFrame(data=stats)\n    df_stats = df_stats.set_index('epoch')\n    return df_stats,model","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:37:28.835624Z","iopub.execute_input":"2023-07-12T14:37:28.836031Z","iopub.status.idle":"2023-07-12T14:37:28.858380Z","shell.execute_reply.started":"2023-07-12T14:37:28.835998Z","shell.execute_reply":"2023-07-12T14:37:28.857282Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"history,model = evaluate_text_classifier(train_dataloader,validation_dataloader,epochs)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:37:31.231450Z","iopub.execute_input":"2023-07-12T14:37:31.231816Z","iopub.status.idle":"2023-07-12T14:40:44.111305Z","shell.execute_reply.started":"2023-07-12T14:37:31.231787Z","shell.execute_reply":"2023-07-12T14:40:44.110205Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:08.\n  Batch    80  of    241.    Elapsed: 0:00:16.\n  Batch   120  of    241.    Elapsed: 0:00:23.\n  Batch   160  of    241.    Elapsed: 0:00:31.\n  Batch   200  of    241.    Elapsed: 0:00:39.\n  Batch   240  of    241.    Elapsed: 0:00:47.\n\n  Average training loss: 0.08\n  Training epcoh took: 0:00:47\n\nRunning Validation...\n  Accuracy: 0.83\n  Validation Loss: 0.62\n  Validation took: 0:00:01\n\n======== Epoch 2 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:08.\n  Batch    80  of    241.    Elapsed: 0:00:16.\n  Batch   120  of    241.    Elapsed: 0:00:23.\n  Batch   160  of    241.    Elapsed: 0:00:31.\n  Batch   200  of    241.    Elapsed: 0:00:39.\n  Batch   240  of    241.    Elapsed: 0:00:47.\n\n  Average training loss: 0.06\n  Training epcoh took: 0:00:47\n\nRunning Validation...\n  Accuracy: 0.83\n  Validation Loss: 0.62\n  Validation took: 0:00:02\n\n======== Epoch 3 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:08.\n  Batch    80  of    241.    Elapsed: 0:00:16.\n  Batch   120  of    241.    Elapsed: 0:00:23.\n  Batch   160  of    241.    Elapsed: 0:00:31.\n  Batch   200  of    241.    Elapsed: 0:00:39.\n  Batch   240  of    241.    Elapsed: 0:00:47.\n\n  Average training loss: 0.07\n  Training epcoh took: 0:00:47\n\nRunning Validation...\n  Accuracy: 0.83\n  Validation Loss: 0.62\n  Validation took: 0:00:01\n\n======== Epoch 4 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:08.\n  Batch    80  of    241.    Elapsed: 0:00:16.\n  Batch   120  of    241.    Elapsed: 0:00:23.\n  Batch   160  of    241.    Elapsed: 0:00:31.\n  Batch   200  of    241.    Elapsed: 0:00:39.\n  Batch   240  of    241.    Elapsed: 0:00:47.\n\n  Average training loss: 0.08\n  Training epcoh took: 0:00:47\n\nRunning Validation...\n  Accuracy: 0.83\n  Validation Loss: 0.62\n  Validation took: 0:00:01\n\nTraining complete!\nTotal training took 0:03:13 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndef prepare_test_dl(tokenizer):\n\n    # Load the dataset into a pandas dataframe.\n    df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", \n                     delimiter='\\t', \n                     header=None, \n                     names=['sentence_source', 'label', 'label_notes', 'sentence'])\n\n    # Report the number of sentences.\n    print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n\n    # Create sentence and label lists\n    sentences = df.sentence.values\n    labels = df.label.values\n\n    # Токенизируем и конвертируем в числовой вид с паддингом\n    input_ids = []\n    attention_masks = []\n    for sent in sentences:\n        encoded_dict = tokenizer.encode_plus(\n                                            sent,              \n                                            add_special_tokens = True, \n                                            max_length = 64,  \n                                            pad_to_max_length = True,\n                                            return_attention_mask = True,\n                                            return_tensors = 'pt',  \n                                             )\n\n        # Сохраняем числовые представление и аттн маск\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Конвертируем списки в тензорыа\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    # Создаем Датасет и Даталодер\n    batch_size = 32\n    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n    prediction_sampler = SequentialSampler(prediction_data)\n    prediction_dataloader = DataLoader(prediction_data, \n                                       sampler=prediction_sampler, \n                                       batch_size=batch_size)\n    \n    return df,prediction_dataloader\n\ndf,test_dataloader = prepare_test_dl(tokeniser)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:03:13.304285Z","iopub.execute_input":"2023-07-12T15:03:13.304684Z","iopub.status.idle":"2023-07-12T15:03:13.438015Z","shell.execute_reply.started":"2023-07-12T15:03:13.304653Z","shell.execute_reply":"2023-07-12T15:03:13.436810Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Number of test sentences: 516\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prediction on test set\nfrom sklearn.metrics import matthews_corrcoef\n\nprint('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n\ndef inference(df,dataloader):\n    \n    ''' Inference on Test Dataloader '''\n    # сохраняем данные из батчев\n\n    model.eval()\n    predictions , true_labels = [], []\n    for batch in test_dataloader:\n\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        with torch.no_grad():\n            outputs = model(b_input_ids, \n                            token_type_ids=None,\n                            attention_mask=b_input_mask)\n\n        logits = outputs[0]\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        predictions.append(logits)\n        true_labels.append(label_ids)\n        \n    ''' Evaluation Metrics '''\n\n    matthews_set = []\n    print('Calculating Matthews Corr. Coef. for each batch...')\n\n    for i in range(len(true_labels)):\n        pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n        matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n        matthews_set.append(matthews)\n        \n    # объединяем предсказании всех батчев (1/0)\n    flat_predictions = np.concatenate(predictions, axis=0)\n    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n    \n    # объединяем все батчи правельных ответов (1/0)\n    flat_true_labels = np.concatenate(true_labels, axis=0)\n    mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n    print('Total MCC: %.3f' % mcc)\n    print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))\n\ninference(df,test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T15:15:39.245623Z","iopub.execute_input":"2023-07-12T15:15:39.245990Z","iopub.status.idle":"2023-07-12T15:15:40.177789Z","shell.execute_reply.started":"2023-07-12T15:15:39.245959Z","shell.execute_reply":"2023-07-12T15:15:40.176593Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Predicting labels for 8,551 test sentences...\nCalculating Matthews Corr. Coef. for each batch...\nTotal MCC: 0.535\nPositive samples: 354 of 516 (68.60%)\n","output_type":"stream"}]}]}